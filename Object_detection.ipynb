{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9973f8b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install --ignore-installed torchvision --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13169bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip uninstall torch \n",
    "# !pip install torch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1f81fbe6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pycocotools\n",
      "  Obtaining dependency information for pycocotools from https://files.pythonhosted.org/packages/24/b2/ef28a34cf6ca50b6b2f7ad81e5837ed45c252ffef22f5a704b94141ea842/pycocotools-2.0.7-cp311-cp311-win_amd64.whl.metadata\n",
      "  Downloading pycocotools-2.0.7-cp311-cp311-win_amd64.whl.metadata (1.1 kB)\n",
      "Requirement already satisfied: matplotlib>=2.1.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from pycocotools) (3.7.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\user\\appdata\\roaming\\python\\python311\\site-packages (from pycocotools) (1.26.4)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from matplotlib>=2.1.0->pycocotools) (1.0.5)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\user\\anaconda3\\lib\\site-packages (from matplotlib>=2.1.0->pycocotools) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from matplotlib>=2.1.0->pycocotools) (4.25.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from matplotlib>=2.1.0->pycocotools) (1.4.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from matplotlib>=2.1.0->pycocotools) (23.0)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\user\\appdata\\roaming\\python\\python311\\site-packages (from matplotlib>=2.1.0->pycocotools) (10.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from matplotlib>=2.1.0->pycocotools) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\user\\anaconda3\\lib\\site-packages (from matplotlib>=2.1.0->pycocotools) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\user\\anaconda3\\lib\\site-packages (from python-dateutil>=2.7->matplotlib>=2.1.0->pycocotools) (1.16.0)\n",
      "Downloading pycocotools-2.0.7-cp311-cp311-win_amd64.whl (85 kB)\n",
      "   ---------------------------------------- 0.0/85.8 kB ? eta -:--:--\n",
      "   ------------------- -------------------- 41.0/85.8 kB 2.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 85.8/85.8 kB 2.4 MB/s eta 0:00:00\n",
      "Installing collected packages: pycocotools\n",
      "Successfully installed pycocotools-2.0.7\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pycocotools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d1cdeed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install Cython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfe252f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch==2.3.1 torchvision==0.18.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49cdcedc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torchvision.transforms import functional as F\n",
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "from torch.utils.data import DataLoader\n",
    "import xml.etree.ElementTree as ET\n",
    "import torchvision.transforms as T\n",
    "\n",
    "from keras.preprocessing.image import img_to_array\n",
    "import os\n",
    "import xml.etree.ElementTree as ET\n",
    "from sklearn.model_selection import train_test_split\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e9976c9",
   "metadata": {},
   "source": [
    "# Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "fcd4f722",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of images: 128\n",
      "Number of annotations: 128\n",
      "Epoch 1 finished with loss 0.30721786618232727\n",
      "Epoch 2 finished with loss 0.16368253529071808\n",
      "Epoch 3 finished with loss 0.11984279751777649\n",
      "Epoch 4 finished with loss 0.07419842481613159\n",
      "Epoch 5 finished with loss 0.029352128505706787\n",
      "Epoch 6 finished with loss 0.057953108102083206\n",
      "Epoch 7 finished with loss 0.13673891127109528\n",
      "Epoch 8 finished with loss 0.05913308635354042\n",
      "Epoch 9 finished with loss 0.07079485803842545\n",
      "Epoch 10 finished with loss 0.0630192831158638\n"
     ]
    }
   ],
   "source": [
    "# Paths to images and annotations\n",
    "image_dir = r\"C:\\Users\\User\\Downloads\\dataset22\\dataset2\\images\"\n",
    "annotation_dir = r\"C:\\Users\\User\\Downloads\\dataset22\\dataset2\\annotations\"\n",
    "\n",
    "# Collect all image and annotation file names\n",
    "image_files = sorted([f for f in os.listdir(image_dir) if f.endswith('.jpg')])\n",
    "annotation_files = sorted([f for f in os.listdir(annotation_dir) if f.endswith('.xml')])\n",
    "\n",
    "# Check if the lists are correctly populated\n",
    "print(f\"Number of images: {len(image_files)}\")\n",
    "print(f\"Number of annotations: {len(annotation_files)}\")\n",
    "\n",
    "# Check if the number of image files matches the number of annotation files\n",
    "assert len(image_files) == len(annotation_files), \"Number of images and annotations do not match\"\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "train_images, test_images, train_annotations, test_annotations = train_test_split(\n",
    "    image_files, annotation_files, train_size=100, test_size=28, random_state=42)\n",
    "\n",
    "# Define the label mapping\n",
    "label_mapping = {\n",
    "    'obj1': 'Water Bottle',\n",
    "    'obj2': 'Milk Bottle',\n",
    "    'obj3': 'Tetra Pack',\n",
    "    'obj4': 'Can'\n",
    "}\n",
    "\n",
    "# Update label_map to use the actual object names\n",
    "label_map = {'Water Bottle': 1, 'Milk Bottle': 2, 'Tetra Pack': 3, 'Can': 4}\n",
    "\n",
    "def parse_annotation(annotation_file):\n",
    "    tree = ET.parse(annotation_file)\n",
    "    root = tree.getroot()\n",
    "    \n",
    "    objects = []\n",
    "    for obj in root.findall('object'):\n",
    "        obj_name = obj.find('name').text\n",
    "        if obj_name in label_mapping:\n",
    "            obj_name = label_mapping[obj_name]\n",
    "        bbox = obj.find('bndbox')\n",
    "        bbox = [int(bbox.find('xmin').text), int(bbox.find('ymin').text),\n",
    "                int(bbox.find('xmax').text), int(bbox.find('ymax').text)]\n",
    "        objects.append((obj_name, bbox))\n",
    "    \n",
    "    return objects\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, image_files, annotation_files, image_dir, annotation_dir, transform=None):\n",
    "        self.image_files = image_files\n",
    "        self.annotation_files = annotation_files\n",
    "        self.image_dir = image_dir\n",
    "        self.annotation_dir = annotation_dir\n",
    "        self.transform = transform\n",
    "        self.label_map = label_map\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.image_dir, self.image_files[idx])\n",
    "        ann_path = os.path.join(self.annotation_dir, self.annotation_files[idx])\n",
    "        \n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        annotations = parse_annotation(ann_path)\n",
    "        \n",
    "        boxes = []\n",
    "        labels = []\n",
    "        for ann in annotations:\n",
    "            if ann[0] not in self.label_map:\n",
    "                print(f\"Unexpected label found: {ann[0]}\")\n",
    "                continue  # Skip unexpected labels\n",
    "            boxes.append(ann[1])\n",
    "            labels.append(self.label_map[ann[0]])\n",
    "        \n",
    "        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "        labels = torch.as_tensor(labels, dtype=torch.int64)\n",
    "        \n",
    "        target = {}\n",
    "        target[\"boxes\"] = boxes\n",
    "        target[\"labels\"] = labels\n",
    "        \n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "        \n",
    "        return img, target\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = CustomDataset(train_images, train_annotations, image_dir, annotation_dir, transform=transforms.ToTensor())\n",
    "test_dataset = CustomDataset(test_images, test_annotations, image_dir, annotation_dir, transform=transforms.ToTensor())\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=2, shuffle=True, collate_fn=lambda x: tuple(zip(*x)))\n",
    "test_loader = DataLoader(test_dataset, batch_size=2, shuffle=False, collate_fn=lambda x: tuple(zip(*x)))\n",
    "\n",
    "# Define the model\n",
    "import torchvision.models.detection\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "\n",
    "def get_model(num_classes):\n",
    "    # Load a pre-trained model\n",
    "    model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "    \n",
    "    # Get the number of input features for the classifier\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    \n",
    "    # Replace the pre-trained head with a new one\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Number of classes (4 objects + 1 background)\n",
    "num_classes = 5\n",
    "\n",
    "# Initialize the model\n",
    "model = get_model(num_classes)\n",
    "\n",
    "# Define the optimizer\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.005, momentum=0.9, weight_decay=0.0005)\n",
    "\n",
    "# Define the learning rate scheduler\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1)\n",
    "\n",
    "# Train the model\n",
    "num_epochs = 10\n",
    "\n",
    "# Check if CUDA is available\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "model.to(device)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for images, targets in train_loader:\n",
    "        images = list(image.to(device) for image in images)\n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "        # Forward pass\n",
    "        loss_dict = model(images, targets)\n",
    "\n",
    "        # Calculate the total loss\n",
    "        losses = sum(loss for loss in loss_dict.values())\n",
    "\n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        losses.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    # Update the learning rate\n",
    "    lr_scheduler.step()\n",
    "\n",
    "    print(f\"Epoch {epoch+1} finished with loss {losses.item()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "023a382e",
   "metadata": {},
   "source": [
    "# Evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a793f58c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.3008\n",
      "Recall: 1.0000\n",
      "F1 Score: 0.4625\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "def evaluate_model(model, data_loader, device, iou_threshold=0.5):\n",
    "    model.eval()\n",
    "    true_positive = 0\n",
    "    false_positive = 0\n",
    "    false_negative = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, targets in data_loader:\n",
    "            images = list(img.to(device) for img in images)\n",
    "            outputs = model(images)\n",
    "            \n",
    "            for target, output in zip(targets, outputs):\n",
    "                true_boxes = target['boxes'].cpu()\n",
    "                true_labels = target['labels'].cpu()\n",
    "                pred_boxes = output['boxes'].cpu()\n",
    "                pred_labels = output['labels'].cpu()\n",
    "\n",
    "                matched_indices = []\n",
    "\n",
    "                for i, true_box in enumerate(true_boxes):\n",
    "                    if true_labels[i] not in pred_labels:\n",
    "                        false_negative += 1\n",
    "                        continue\n",
    "\n",
    "                    ious = torchvision.ops.box_iou(true_box.unsqueeze(0), pred_boxes).squeeze(0)\n",
    "                    best_iou_idx = torch.argmax(ious)\n",
    "                    if ious[best_iou_idx] >= iou_threshold:\n",
    "                        if best_iou_idx not in matched_indices:\n",
    "                            matched_indices.append(best_iou_idx)\n",
    "                            true_positive += 1\n",
    "                        else:\n",
    "                            false_negative += 1\n",
    "                    else:\n",
    "                        false_negative += 1\n",
    "\n",
    "                false_positive += len(pred_boxes) - len(matched_indices)\n",
    "\n",
    "    precision = true_positive / (true_positive + false_positive) if true_positive + false_positive > 0 else 0\n",
    "    recall = true_positive / (true_positive + false_negative) if true_positive + false_negative > 0 else 0\n",
    "    f1 = 2 * (precision * recall) / (precision + recall) if precision + recall > 0 else 0\n",
    "\n",
    "    return precision, recall, f1\n",
    "\n",
    "# Load the model to the appropriate device\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "model.to(device)\n",
    "\n",
    "# Create the test data loader\n",
    "test_loader = DataLoader(test_dataset, batch_size=2, shuffle=False, collate_fn=lambda x: tuple(zip(*x)))\n",
    "\n",
    "# Evaluate the model\n",
    "precision, recall, f1 = evaluate_model(model, test_loader, device)\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4809bad",
   "metadata": {},
   "source": [
    "# class wise precision recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5bf62f97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class 4:\n",
      "  Precision: 0.1789\n",
      "  Recall: 1.0000\n",
      "  F1 Score: 0.3036\n",
      "Class 3:\n",
      "  Precision: 0.2289\n",
      "  Recall: 1.0000\n",
      "  F1 Score: 0.3725\n",
      "Class 1:\n",
      "  Precision: 0.2899\n",
      "  Recall: 1.0000\n",
      "  F1 Score: 0.4494\n",
      "Class 2:\n",
      "  Precision: 0.2466\n",
      "  Recall: 1.0000\n",
      "  F1 Score: 0.3956\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "from collections import defaultdict\n",
    "\n",
    "def evaluate_model_classwise(model, data_loader, device, iou_threshold=0.5):\n",
    "    model.eval()\n",
    "    \n",
    "    class_metrics = defaultdict(lambda: {'true_positive': 0, 'false_positive': 0, 'false_negative': 0})\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, targets in data_loader:\n",
    "            images = list(img.to(device) for img in images)\n",
    "            outputs = model(images)\n",
    "\n",
    "            for target, output in zip(targets, outputs):\n",
    "                true_boxes = target['boxes'].cpu()\n",
    "                true_labels = target['labels'].cpu()\n",
    "                pred_boxes = output['boxes'].cpu()\n",
    "                pred_labels = output['labels'].cpu()\n",
    "\n",
    "                matched_indices = set()\n",
    "\n",
    "                for i, true_box in enumerate(true_boxes):\n",
    "                    true_label = true_labels[i].item()\n",
    "                    if true_label not in pred_labels:\n",
    "                        class_metrics[true_label]['false_negative'] += 1\n",
    "                        continue\n",
    "\n",
    "                    ious = torchvision.ops.box_iou(true_box.unsqueeze(0), pred_boxes).squeeze(0)\n",
    "                    best_iou_idx = torch.argmax(ious)\n",
    "                    if ious[best_iou_idx] >= iou_threshold:\n",
    "                        if best_iou_idx not in matched_indices:\n",
    "                            matched_indices.add(best_iou_idx)\n",
    "                            class_metrics[true_label]['true_positive'] += 1\n",
    "                        else:\n",
    "                            class_metrics[true_label]['false_negative'] += 1\n",
    "                    else:\n",
    "                        class_metrics[true_label]['false_negative'] += 1\n",
    "\n",
    "                for i, pred_label in enumerate(pred_labels):\n",
    "                    if i not in matched_indices:\n",
    "                        class_metrics[pred_label.item()]['false_positive'] += 1\n",
    "\n",
    "    precisions = {}\n",
    "    recalls = {}\n",
    "    f1_scores = {}\n",
    "\n",
    "    for cls, metrics in class_metrics.items():\n",
    "        tp = metrics['true_positive']\n",
    "        fp = metrics['false_positive']\n",
    "        fn = metrics['false_negative']\n",
    "        precision = tp / (tp + fp) if tp + fp > 0 else 0\n",
    "        recall = tp / (tp + fn) if tp + fn > 0 else 0\n",
    "        f1 = 2 * (precision * recall) / (precision + recall) if precision + recall > 0 else 0\n",
    "        precisions[cls] = precision\n",
    "        recalls[cls] = recall\n",
    "        f1_scores[cls] = f1\n",
    "\n",
    "    return precisions, recalls, f1_scores\n",
    "\n",
    "# Load the model to the appropriate device\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "model.to(device)\n",
    "\n",
    "# Create the test data loader\n",
    "test_loader = DataLoader(test_dataset, batch_size=2, shuffle=False, collate_fn=lambda x: tuple(zip(*x)))\n",
    "\n",
    "# Evaluate the model\n",
    "precisions, recalls, f1_scores = evaluate_model_classwise(model, test_loader, device)\n",
    "\n",
    "for cls in precisions.keys():\n",
    "    print(f\"Class {cls}:\")\n",
    "    print(f\"  Precision: {precisions[cls]:.4f}\")\n",
    "    print(f\"  Recall: {recalls[cls]:.4f}\")\n",
    "    print(f\"  F1 Score: {f1_scores[cls]:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b9442ae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
